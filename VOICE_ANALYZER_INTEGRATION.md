# üé§ Voice Analyzer Integration - Echtzeit-Sprachanalyse

## ‚ú® **NEUE FEATURES**

### **1. Audio Feature Extraction (Meyda.js)**
‚úÖ **Lautst√§rke (RMS)** - Zu leise? Zu laut?  
‚úÖ **Energie** - Energieniveau der Stimme  
‚úÖ **Klarheit (Zero Crossing Rate)** - Stimmqualit√§t  
‚úÖ **Spektraler Schwerpunkt** - "Helligkeit" der Stimme

### **2. Speech Recognition (Web Speech API)**
‚úÖ **Worte pro Minute (WPM)** - Sprechgeschwindigkeit  
‚úÖ **F√ºllw√∂rter-Erkennung** - "√§hm", "also", "halt", etc.  
‚úÖ **Echtzeit-Transkription** - Gesprochene Worte werden erkannt  
‚úÖ **Confidence** - Wie klar/deutlich wird gesprochen?

### **3. Berechnete Metriken**
‚úÖ **Pausen-Detektion** - Anzahl, L√§nge, Durchschnitt  
‚úÖ **Redefluss-Score** - Kombination aus WPM, Pausen, F√ºllw√∂rtern  
‚úÖ **Speech Flow** - Gesamtbewertung 0-100  
‚úÖ **Flags** - Warnungen (ZU_SCHNELL, VIELE_FUELLWOERTER, etc.)

---

## üèóÔ∏è **ARCHITEKTUR**

```
LiveSession.jsx
    ‚Üì
    ‚îú‚îÄ MediaPipe Service (Pose, Face Mesh, Hands)
    ‚îú‚îÄ Voice Analyzer Service (NEW!)
    ‚îÇ   ‚îú‚îÄ Web Audio API + Meyda.js
    ‚îÇ   ‚îú‚îÄ Web Speech API
    ‚îÇ   ‚îî‚îÄ Feature Aggregation
    ‚îî‚îÄ Analysis Aggregator
        ‚Üì
    Backend API ‚Üí ChatGPT
```

---

## üì¶ **NEUE DATEIEN**

### **1. `frontend/src/services/VoiceAnalyzer.js`**

**Klasse:** `VoiceAnalyzer` (Singleton)

**Hauptmethoden:**
```javascript
// Initialisierung
await voiceAnalyzer.initialize(stream, 'de-DE');

// Echtzeit-Daten abrufen
const analysis = voiceAnalyzer.getRealtimeAnalysis();
// Returns: { volume, energy, clarity, speech, pauses, overall }

// Finale Daten f√ºr Backend
const data = voiceAnalyzer.exportForBackend();

// Stoppen
voiceAnalyzer.stop();

// Reset f√ºr neue Session
voiceAnalyzer.reset();
```

**Features:**
- ‚úÖ **Meyda.js** f√ºr Audio-Feature-Extraction
- ‚úÖ **Web Speech API** f√ºr Speech-to-Text
- ‚úÖ **Pausen-Detektion** (RMS < 0.01 = Stille)
- ‚úÖ **F√ºllw√∂rter-Liste** (15 deutsche F√ºllw√∂rter)
- ‚úÖ **Score-Berechnung** (0-100 f√ºr jedes Feature)
- ‚úÖ **Flags-Generierung** (Warnungen bei Auff√§lligkeiten)

---

## üîß **INTEGRATION IN LIVESESSION.JSX**

### **Import:**
```javascript
import voiceAnalyzer from '../services/VoiceAnalyzer';
```

### **State:**
```javascript
const [voiceAnalyzerInitialized, setVoiceAnalyzerInitialized] = useState(false);
const voiceUpdateIntervalRef = useRef(null);
```

### **Initialisierung (in `initMediaPipe`):**
```javascript
if (microphoneOn && streamRef.current) {
  await voiceAnalyzer.initialize(streamRef.current, 'de-DE');
  setVoiceAnalyzerInitialized(true);
  
  // Voice Score Update Interval (alle 2 Sekunden)
  voiceUpdateIntervalRef.current = setInterval(() => {
    updateVoiceScore();
  }, 2000);
}
```

### **Voice Score Update:**
```javascript
const updateVoiceScore = () => {
  const voiceAnalysis = voiceAnalyzer.getRealtimeAnalysis();
  
  setScores(prevScores => ({
    ...prevScores,
    stimme: voiceAnalysis.overall  // 0-100
  }));
};
```

### **Finale Daten Export (in `handleStop`):**
```javascript
const mediaPipeData = analysisAggregator.exportForBackend();
const voiceData = voiceAnalyzer.exportForBackend();

const analysisData = {
  ...mediaPipeData,
  ...voiceData,
  summary: {
    ...mediaPipeData.summary,
    voiceScore: voiceData.voice.overall
  }
};

// An Backend senden
await apiClient.post('/analyze/save', analysisData);
```

---

## üìä **DATEN-FORMAT**

### **Realtime Analysis (`getRealtimeAnalysis()`):**

```javascript
{
  volume: {
    current: 0.45,       // Aktuelle Lautst√§rke
    average: 0.42,       // Durchschnitt
    score: 85            // Score 0-100
  },
  energy: {
    average: 0.56,
    score: 90
  },
  clarity: {
    average: 0.72,       // Zero Crossing Rate
    score: 100
  },
  speech: {
    wordsPerMinute: 145,           // WPM (Optimal: 120-160)
    fillerWordsPerMinute: 2.3,     // F√ºllw√∂rter/Min
    fillerWordCount: 8,            // Gesamt
    totalWords: 245,               // Gesamt gesprochene Worte
    score: 82
  },
  pauses: {
    count: 23,                     // Anzahl Pausen
    averageLength: 1.2,            // Sekunden
    longestPause: 3.5,             // Sekunden
    score: 78
  },
  overall: 82  // Gesamter Voice Score (0-100)
}
```

### **Backend Export (`exportForBackend()`):**

```javascript
{
  voice: {
    volume: { current, average, score },
    energy: { average, score },
    clarity: { average, score },
    speech: { wordsPerMinute, fillerWordsPerMinute, fillerWordCount, totalWords, score },
    pauses: { count, averageLength, longestPause, score },
    overall: 82,
    flags: ['ZU_VIELE_FUELLWOERTER', 'ZU_SCHNELL']
  },
  raw: {
    transcripts: [
      { timestamp: 1234567890, text: 'Hello world', confidence: 0.95 },
      // ...
    ],
    fillerWords: [
      { timestamp: 1234567890, word: '√§hm', context: 'Hello √§hm world' },
      // ...
    ]
  }
}
```

---

## üéØ **SCORE-BERECHNUNG**

### **Speech Flow Score (0-100):**

**Basis:** 100 Punkte

**Abz√ºge:**
- **WPM < 100:** -15 Punkte (zu langsam)
- **WPM > 180:** -20 Punkte (zu schnell)
- **WPM < 80:** -25 Punkte (sehr langsam)
- **WPM > 200:** -30 Punkte (sehr schnell)
- **F√ºllw√∂rter > 3/Min:** -10 Punkte
- **F√ºllw√∂rter > 5/Min:** -20 Punkte
- **F√ºllw√∂rter > 8/Min:** -30 Punkte
- **Pausen-Score:** Mittelwert mit Pausen-Score

**Optimal:**
- **WPM:** 120-160
- **F√ºllw√∂rter:** < 3 pro Minute
- **Pausen:** 1-2 Sekunden durchschnittlich

### **Volume Score (0-100):**
- **< 0.1:** 40 (sehr leise)
- **0.1-0.2:** 60 (zu leise)
- **0.3-0.6:** 100 (optimal)
- **0.7-0.8:** 70 (zu laut)
- **> 0.8:** 50 (sehr laut)

### **Pausen Score (0-100):**
**Basis:** 100 Punkte

**Abz√ºge:**
- **Durchschnitt < 0.5s:** -10 (zu kurz)
- **Durchschnitt > 3s:** -20 (zu lang)
- **L√§ngste Pause > 5s:** -15 (sehr lang)

---

## üö® **FLAGS (Warnungen)**

Automatisch generierte Warnungen bei Auff√§lligkeiten:

| Flag | Bedingung | Bedeutung |
|------|-----------|-----------|
| `ZU_LEISE` | Volume Score < 60 | Zu leise sprechen |
| `ZU_LAUT` | Volume > 0.7 | Zu laut sprechen |
| `ZU_LANGSAM` | WPM < 100 | Zu langsam sprechen |
| `ZU_SCHNELL` | WPM > 180 | Zu schnell sprechen |
| `VIELE_FUELLWOERTER` | F√ºllw√∂rter > 5/Min | Zu viele F√ºllw√∂rter |
| `LANGE_PAUSEN` | Durchschnitt > 3s | Pausen zu lang |
| `WENIGE_PAUSEN` | < 5 Pausen bei > 100 Worten | Zu wenige Pausen |

---

## üé§ **F√úLLW√ñRTER-LISTE**

**Deutsch (15 F√ºllw√∂rter):**
```javascript
const fillerWords = [
  '√§hm', 'ehm', '√∂hm',
  'also', 'halt', 'quasi',
  'sozusagen', 'irgendwie',
  'gewisserma√üen', 'im prinzip',
  'praktisch', 'theoretisch',
  'einfach', 'genau', 'eben'
];
```

**Erweiterbar f√ºr andere Sprachen:**
```javascript
// Englisch
const fillerWordsEN = ['um', 'uh', 'like', 'you know', 'actually', ...];

// Franz√∂sisch
const fillerWordsFR = ['euh', 'ben', 'quoi', 'genre', ...];
```

---

## üß™ **TESTING**

### **Lokaler Test:**
```bash
cd frontend
npm run dev
```

**Navigiere zu:** http://localhost:5173/session-prepare

### **Test-Szenario:**

1. **Starte Analyse** ‚Üí Kamera/Mikrofon erlauben
2. **Warte 5 Sekunden** ‚Üí MediaPipe + Voice Analyzer laden
3. **Spreche laut und deutlich:**
   - "Hallo, ich teste die Voice Analysis."
   - "Ich spreche jetzt etwas schneller."
   - "√Ñhm... und jetzt mit F√ºllw√∂rtern."
4. **Pr√ºfe Browser Console (F12):**

```javascript
// Sollte alle 2 Sekunden erscheinen:
üé§ Voice Score: {
  overall: 82,
  wpm: 145,
  fillerWords: 3,
  pauses: 5
}

// Alle 4 Sekunden:
üé§ Audio Features: {
  rms: 0.452,
  energy: 0.563,
  zcr: 0.721,
  pauses: 5
}

// Bei Speech Recognition:
üó£Ô∏è Speech: {
  transcript: "Hallo ich teste die Voice Analysis",
  confidence: "95%",
  fillerWords: 0,
  totalWords: 6
}
```

5. **Pr√ºfe UI:**
   - **Stimme-Score-Balken** aktualisiert sich alle 2 Sekunden
   - Wert sollte zwischen 60-90 liegen (bei normalem Sprechen)

6. **Stoppe Analyse** ‚Üí Pr√ºfe Console:

```javascript
üìä Finale Analyse: {
  voice: {
    overall: 82,
    flags: ['ZU_VIELE_FUELLWOERTER'],
    // ...
  },
  raw: {
    transcripts: [...],
    fillerWords: [...]
  }
}
```

---

## üêõ **TROUBLESHOOTING**

### **Problem: Mikrofon wird nicht erkannt**

**Symptom:** Voice Analyzer startet nicht, keine Audio-Features

**L√∂sung:**
1. Pr√ºfe Browser-Berechtigung (Mikrofon erlaubt?)
2. Pr√ºfe: `streamRef.current` enth√§lt Audio-Track?
3. Console-Log: `‚ö†Ô∏è Voice Analyzer konnte nicht initialisiert werden`

```javascript
// Debug:
console.log('Stream:', streamRef.current);
console.log('Audio Tracks:', streamRef.current?.getAudioTracks());
```

---

### **Problem: Speech Recognition funktioniert nicht**

**Symptom:** Keine Transkripte, keine F√ºllw√∂rter

**L√∂sung:**
1. **Browser-Support pr√ºfen:**
   - ‚úÖ Chrome/Edge: Funktioniert
   - ‚ùå Firefox: Nicht unterst√ºtzt
   - ‚úÖ Safari: Funktioniert (mit Einschr√§nkungen)

```javascript
// Check Support:
const isSupported = !!(window.SpeechRecognition || window.webkitSpeechRecognition);
console.log('Speech Recognition supported:', isSupported);
```

2. **Sprache pr√ºfen:**
   - Aktuell: `de-DE` (Deutsch)
   - √Ñndern in `VoiceAnalyzer.initialize(stream, 'en-US')`

3. **Mikrofon-Qualit√§t:**
   - Schlechte Mikrofone ‚Üí niedrige Confidence
   - Hintergrundger√§usche ‚Üí falsche Erkennung

---

### **Problem: Voice Score bleibt bei 29**

**Symptom:** Score aktualisiert sich nicht

**L√∂sung:**
1. Pr√ºfe: `voiceAnalyzerInitialized === true`?
2. Pr√ºfe: `voiceUpdateIntervalRef.current` existiert?
3. Console: Erscheint `üé§ Voice Score:` alle 2 Sekunden?

```javascript
// Debug:
console.log('Voice Analyzer initialized:', voiceAnalyzerInitialized);
console.log('Update Interval:', voiceUpdateIntervalRef.current);
console.log('Voice Analyzer running:', voiceAnalyzer.isRunning);
```

---

### **Problem: Zu viele/wenige F√ºllw√∂rter erkannt**

**Symptom:** F√ºllw√∂rter-Count unrealistisch

**L√∂sung:**
1. **F√ºllw√∂rter-Liste anpassen** (`VoiceAnalyzer.js`):

```javascript
this.fillerWordList = [
  '√§hm', 'ehm', '√∂hm',  // Nur diese 3
  // Rest auskommentieren f√ºr weniger False Positives
];
```

2. **Confidence-Threshold erh√∂hen:**

```javascript
// In handleSpeechResult:
if (isFinal && confidence > 0.8) {  // Nur >80% Confidence
  // ... F√ºllw√∂rter erkennen
}
```

---

### **Problem: Pausen werden nicht erkannt**

**Symptom:** Pausen-Count = 0

**L√∂sung:**
1. **Pause-Threshold anpassen:**

```javascript
this.pauseThreshold = 0.02;  // Von 0.01 auf 0.02 erh√∂hen
```

2. **Min. Pausen-L√§nge reduzieren:**

```javascript
this.minPauseLength = 0.3;  // Von 0.5 auf 0.3 Sekunden
```

3. **Mikrofon-Empfindlichkeit:**
   - Zu empfindlich ‚Üí Keine Stille erkannt
   - L√∂sung: Threshold erh√∂hen

---

## üìà **PERFORMANCE**

### **Ressourcen-Verbrauch:**

| Feature | CPU | Memory | Network |
|---------|-----|--------|---------|
| **Meyda.js** | ~2-5% | ~10 MB | 0 KB |
| **Web Speech API** | ~1-3% | ~5 MB | ~10 KB/s |
| **Gesamt** | ~3-8% | ~15 MB | ~10 KB/s |

### **Optimierungen:**

1. **Buffer Size:** 512 (Balance zwischen Latenz & Performance)
2. **Update Interval:** 2 Sekunden (nicht zu h√§ufig)
3. **Feature Extractors:** Nur 4 (nicht alle 25 Meyda-Features)

---

## üöÄ **DEPLOYMENT**

```bash
git add .
git commit -m "üé§ Voice Analyzer: Echtzeit-Sprachanalyse mit Meyda.js + Speech API"
git push origin main
```

**Vercel:** Automatisches Deployment

**Dependencies:**
```json
{
  "meyda": "^5.7.0"
}
```

---

## üìä **CHATGPT INTEGRATION (Backend)**

Die Voice-Daten werden an das Backend gesendet und k√∂nnen von ChatGPT analysiert werden:

```javascript
// Backend: routes/analyze.js
router.post('/save', authenticateToken, async (req, res) => {
  const { voice, raw } = req.body;
  
  // ChatGPT Prompt
  const prompt = `
    Analysiere diese Stimm-Daten:
    - Sprechgeschwindigkeit: ${voice.speech.wordsPerMinute} WPM
    - F√ºllw√∂rter: ${voice.speech.fillerWordCount}
    - Pausen: ${voice.pauses.count} (√ò ${voice.pauses.averageLength}s)
    - Lautst√§rke: ${voice.volume.average}
    - Overall Score: ${voice.overall}
    - Flags: ${voice.flags.join(', ')}
    
    Gib konstruktives Feedback!
  `;
  
  const feedback = await chatGPT.analyze(prompt);
  
  res.json({ feedback });
});
```

---

## üéØ **N√ÑCHSTE SCHRITTE**

### **Phase 1: ‚úÖ Basis-Integration** (DONE)
- ‚úÖ Meyda.js installiert
- ‚úÖ VoiceAnalyzer.js erstellt
- ‚úÖ LiveSession.jsx integriert
- ‚úÖ Realtime Score-Updates

### **Phase 2: Backend-Integration** (TODO)
- ‚è≥ Backend-Route f√ºr Voice-Daten
- ‚è≥ ChatGPT Feedback-Generierung
- ‚è≥ Speicherung in Datenbank

### **Phase 3: UI-Verbesserungen** (TODO)
- ‚è≥ Detaillierte Voice-Statistiken anzeigen
- ‚è≥ F√ºllw√∂rter-Liste in UI
- ‚è≥ Transkript-Anzeige
- ‚è≥ Echtzeit-WPM-Counter

### **Phase 4: Erweiterungen** (FUTURE)
- üìÖ Mehrsprachigkeit (EN, FR, ES)
- üìÖ Stottern-Erkennung
- üìÖ Tonh√∂hen-Analyse
- üìÖ Emotionserkennung (Stimmlage)

---

**Status:** ‚úÖ **READY FOR TESTING**

**Erstellt:** 2025-01-05  
**Version:** 1.0

